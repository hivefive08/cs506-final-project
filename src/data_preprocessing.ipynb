{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "In this project, we processed the movie review dataset to prepare it for sentiment analysis. The goal was to clean, standardize, and convert the text data into a format suitable for machine learning models. The following steps were taken to achieve this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hangy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hangy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hangy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hangy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data files\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('../data/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning\n",
    "- We removed HTML tags, special characters, and numbers from the reviews to eliminate noise. This was done using regular expressions to retain only alphabetic characters in the text.\n",
    "- All text was converted to lowercase to ensure uniformity and prevent the model from treating words with different cases as separate entities (e.g., \"Great\" vs. \"great\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  one of the other reviewers has mentioned that ...  \n",
      "1  a wonderful little production the filming tech...  \n",
      "2  i thought this was a wonderful way to spend ti...  \n",
      "3  basically theres a family where a little boy j...  \n",
      "4  petter matteis love in the time of money is a ...  \n"
     ]
    }
   ],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the review column\n",
    "data['cleaned_review'] = data['review'].apply(clean_text)\n",
    "\n",
    "# Display the first few cleaned reviews\n",
    "print(data[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "- We split each cleaned review into individual words (tokens) using NLTK’s word tokenizer. This step helped us break down each review into a list of words, making it easier to manipulate and analyze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0  one of the other reviewers has mentioned that ...   \n",
      "1  a wonderful little production the filming tech...   \n",
      "2  i thought this was a wonderful way to spend ti...   \n",
      "3  basically theres a family where a little boy j...   \n",
      "4  petter matteis love in the time of money is a ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [one, of, the, other, reviewers, has, mentione...  \n",
      "1  [a, wonderful, little, production, the, filmin...  \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...  \n",
      "3  [basically, theres, a, family, where, a, littl...  \n",
      "4  [petter, matteis, love, in, the, time, of, mon...  \n"
     ]
    }
   ],
   "source": [
    "# Tokenize the cleaned reviews\n",
    "data['tokens'] = data['cleaned_review'].apply(word_tokenize)\n",
    "\n",
    "# Display the first few tokenized reviews\n",
    "print(data[['cleaned_review', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stop Words Removal\n",
    "- We removed common stop words such as \"the\", \"is\", \"and\", etc., using NLTK’s built-in stop word list. Stop words generally do not contribute much to sentiment classification, so removing them helped to reduce noise and improve the quality of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens  \\\n",
      "0  [one, of, the, other, reviewers, has, mentione...   \n",
      "1  [a, wonderful, little, production, the, filmin...   \n",
      "2  [i, thought, this, was, a, wonderful, way, to,...   \n",
      "3  [basically, theres, a, family, where, a, littl...   \n",
      "4  [petter, matteis, love, in, the, time, of, mon...   \n",
      "\n",
      "                                 tokens_no_stopwords  \n",
      "0  [one, reviewers, mentioned, watching, oz, epis...  \n",
      "1  [wonderful, little, production, filming, techn...  \n",
      "2  [thought, wonderful, way, spend, time, hot, su...  \n",
      "3  [basically, theres, family, little, boy, jake,...  \n",
      "4  [petter, matteis, love, time, money, visually,...  \n"
     ]
    }
   ],
   "source": [
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Apply stop words removal to the tokens\n",
    "data['tokens_no_stopwords'] = data['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few tokenized reviews without stop words\n",
    "print(data[['tokens', 'tokens_no_stopwords']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization\n",
    "- We applied lemmatization using NLTK’s WordNetLemmatizer to reduce each word to its base dictionary form, considering its context and part of speech. For example, \"running\" was converted to \"run\", and \"better\" was converted to \"good\". Lemmatization helped standardize words while preserving their true meaning and context, which is crucial for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 tokens_no_stopwords  \\\n",
      "0  [one, reviewers, mentioned, watching, oz, epis...   \n",
      "1  [wonderful, little, production, filming, techn...   \n",
      "2  [thought, wonderful, way, spend, time, hot, su...   \n",
      "3  [basically, theres, family, little, boy, jake,...   \n",
      "4  [petter, matteis, love, time, money, visually,...   \n",
      "\n",
      "                                   lemmatized_tokens  \n",
      "0  [one, reviewer, mentioned, watching, oz, episo...  \n",
      "1  [wonderful, little, production, filming, techn...  \n",
      "2  [thought, wonderful, way, spend, time, hot, su...  \n",
      "3  [basically, there, family, little, boy, jake, ...  \n",
      "4  [petter, matteis, love, time, money, visually,...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to apply lemmatization\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization to the tokens without stop words\n",
    "data['lemmatized_tokens'] = data['tokens_no_stopwords'].apply(lemmatize_words)\n",
    "\n",
    "# Display the first few lemmatized reviews\n",
    "print(data[['tokens_no_stopwords', 'lemmatized_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Final Cleaned Review Preparation\n",
    "After lemmatization, the cleaned and lemmatized tokens were joined back together to form a final processed version of each review. This step was necessary to convert the list of tokens back into strings, which can be used for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Join lemmatized tokens back into a single string for each review\n",
    "data['final_review'] = data['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
    "\n",
    "# Fit and transform the data\n",
    "X = tfidf.fit_transform(data['final_review'])\n",
    "\n",
    "# Display the shape of the resulting TF-IDF matrix\n",
    "print(\"TF-IDF Matrix Shape:\", X.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
